{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c37cc3",
   "metadata": {},
   "source": [
    "Import Important Libraries and Read in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e0025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import random as rnd\n",
    "rnd.seed()\n",
    "G = nx.read_graphml('graph.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26dcf4c",
   "metadata": {},
   "source": [
    "You can try to plot the graph if you want but since this is probably a massive graph, It might take up to much memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b132c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de74d2",
   "metadata": {},
   "source": [
    "Here we are getting the the number of nodes and edges and the mean degree. You can use the number of nodes and edges to check that you graphml file was exported and read correctly. we also get the mean degree which will provide a littel bit of info about the degree distribution of the nodes in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ec831",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.nodes\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "kmean = (2*m)/n\n",
    "\n",
    "print(f'number of nodes, n  = {n}')\n",
    "print(f'number of edges, m  = {m}')\n",
    "print(f'mean degree,    <k> = %5.2f' % kmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492627d",
   "metadata": {},
   "source": [
    "Here we can vizualize what the degree distribution is looking like in the network for both the in and out degrees. This helpful in getting an idea of the connectivness of the network and deteting if there are any nodes that are more highly conneted than others. These highly connected nodes are the nodes we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d5db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2CCDF(kins,kouts):\n",
    "    # input : two lists of in- and out-degrees\n",
    "    # output: a plot showing CCDFs of the in- and out-degree distributions Pr(K>=k) for k>=1\n",
    "    \n",
    "    kin_max  = max(kins)\n",
    "    kout_max = max(kouts)\n",
    "\n",
    "    # histograms\n",
    "    icounts, ibins = np.histogram(kins, bins=[i for i in range(kin_max+2)], density=True)\n",
    "    icumcounts = np.cumsum(icounts)\n",
    "    icumcounts = np.insert(icumcounts,0,0)\n",
    "    ocounts, obins = np.histogram(kouts, bins=[i for i in range(kout_max+2)], density=True)\n",
    "    ocumcounts = np.cumsum(ocounts)\n",
    "    ocumcounts = np.insert(ocumcounts,0,0)\n",
    "\n",
    "    # plots\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111) # put multiple \n",
    "    plt.loglog(obins[1:-1], 1-ocumcounts[1:-1], 'bo', alpha=0.5, label='out-degree')\n",
    "    plt.loglog(ibins[1:-1], 1-icumcounts[1:-1], 'rs', alpha=0.5, label='in-degree')\n",
    "    plt.title('CCDF, in- and out-degrees (loglog)')\n",
    "    plt.xlabel('Degree, k')\n",
    "    plt.ylabel('Pr(K>=k)')\n",
    "    plt.legend(loc='upper right');\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5013d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "# print(G.out_degree())\n",
    "kin_mean = (m)/n\n",
    "kout_mean = (m)/n\n",
    "dgh = nx.degree_histogram(G)\n",
    "md = 0\n",
    "sum = 0\n",
    "ind = 0\n",
    "\n",
    "\n",
    "lto = G.out_degree()\n",
    "l = len(lto)\n",
    "nlto = []\n",
    "for i in range(l):\n",
    "    \n",
    "    nlto.append(lto[i])\n",
    "    \n",
    "ilto = G.out_degree()\n",
    "l = len(lto)\n",
    "inlto = []\n",
    "for i in range(l):\n",
    "    inlto.append(ilto[i])\n",
    "    \n",
    "lti = G.in_degree()\n",
    "l = len(lti)\n",
    "nlti = []\n",
    "for i in range(l):\n",
    "    nlti.append(lti[i])\n",
    "md = 0\n",
    "sum = 0\n",
    "ind = 0\n",
    "hsum = 0\n",
    "for i in range(len(inlto)):\n",
    "    hsum = hsum + inlto[i]\n",
    "while(sum < (hsum/2)):\n",
    "    ind = ind +1\n",
    "    l = len(dgh)\n",
    "    m = max(dgh)\n",
    "    sum = sum + m\n",
    "    j = 0\n",
    "    for i in range(l-1):\n",
    "        if m == inlto[i]:\n",
    "            inlto.pop(i)\n",
    "\n",
    "            \n",
    "print(f'number of nodes, n  = {n}')\n",
    "print(f'number of edges, m  = {m}')\n",
    "print(f'\\nmean(k_in)  = %5.2f' % kin_mean)\n",
    "print(f'mean(k_out) = %5.2f' % kout_mean)\n",
    "print(f'\\nsmallest num for 50%  = {ind} of {n} nodes')\n",
    "plot_2CCDF(nlti,nlto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2367f3",
   "metadata": {},
   "source": [
    "These statistics will again provide us information on how connected this network is and how well/quickf information would move through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7645cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diameter = nx.diameter(G)\n",
    "ellmean = nx.average_shortest_path_length(G)\n",
    "C = nx.transitivity(G)\n",
    "h = nx.number_connected_components(G)\n",
    "print(f'diameter = {diameter}')\n",
    "print(f'mean geodesic distance, <ell> = %5.2f' % ellmean)\n",
    "print(f'clustering coefficient, C     = %5.2f' % C)\n",
    "print(f'number of components,   h     =  {h}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec610c",
   "metadata": {},
   "source": [
    "Here we check the obustness of the network with three applicable metrics from the paper: \"The Rationality of Four Metrics of Network Robustness: A Viewpoint of Robust Growth of Generalized Meshes\" found here https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0161077\n",
    "\n",
    "Below are the docs for the three metrics for more information:\n",
    "\n",
    "Algebraic Connectivity: https://networkx.org/documentation/stable/reference/generated/networkx.linalg.algebraicconnectivity.algebraic_connectivity.html\n",
    "\n",
    "Betweeness Centrality: https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.edge_betweenness_centrality.html\n",
    "\n",
    "Global Efficency: https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.efficiency_measures.global_efficiency.html#networkx.algorithms.efficiency_measures.global_efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad01bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = nx.algebraic_connectivity(G)\n",
    "ec = nx.edge_betweenness_centrality(G)\n",
    "ge = round(nx.global_efficiency(G), 5)\n",
    "\n",
    "print(f'algebraic connectivity = {ac}')\n",
    "print(f'edge betweeness centrality = {ec}')\n",
    "print(f'Global efficency = {ge}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b2ea2",
   "metadata": {},
   "source": [
    "The following networkx functions are all viable measure to look at the centrality of nodes. These will be usefull in identifying the big players in the network so that we can futher study what makes them so essential to the network. the voterank function at the bottom provides a list of the top most influential nodes in the network which again provides us information similar to the other statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c721026",
   "metadata": {},
   "outputs": [],
   "source": [
    "idc = nx.in_degree_centrality(G)\n",
    "odc = nx.out_degree_centrality(G)\n",
    "cfcc = nx.current_flow_closeness_centrality(G)\n",
    "bc = nx.betweenness_centrality(G)\n",
    "lc = nx.load_centrality(G)\n",
    "vr = nx.voterank(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ac95d",
   "metadata": {},
   "source": [
    "Below are a couple of helper functions used to check for and find nodes in the graph. Below that is a script you can run to query a subgraph from the graphml file so that you can run this network analysis on a smaller frame of reference if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNodeInGraph(G, ID):\n",
    "    for node in G.nodes:\n",
    "        if node == ID:\n",
    "            return node\n",
    "           \n",
    "def isNodeInGraph(G, ID):\n",
    "    for node in G.nodes:\n",
    "        if node == ID:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff667585",
   "metadata": {},
   "outputs": [],
   "source": [
    "Id = \"\"\n",
    "r = 1\n",
    "\n",
    "\n",
    "n = findNodeInGraph(G, Id)\n",
    "subG = nx.ego_graph(G, n, r, True, True, None)\n",
    "\n",
    "nx.draw(subG)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ff2df",
   "metadata": {},
   "source": [
    "Here we have the Degree Corrected Stochasitc Block Model which is detailed in this paper :\n",
    "http://www.stat.yale.edu/~hz68/DCBM-aos.pdf\n",
    "\n",
    "This Algorithm will take a long time to run and will require a lot of computational resources, but it is a community detection algorithm that uses the optimization of log likelyhood function to find the correct partitions and correctly identify communities. This code is from some of my coursework so this should be used as a tool and less to be built into a the product itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_z(n,c):\n",
    "    # input  : number of nodes n, and number of groups c\n",
    "    # output : returns a random partition in a dictionary z, where z_i = Uniform(0,c-1)\n",
    "\n",
    "    import random as rnd\n",
    "    rnd.seed()\n",
    "    \n",
    "    z = dict()\n",
    "\n",
    " \n",
    "\n",
    "    for i in range(n):\n",
    "        z.update({i: np.random.choice(c, 1)[0]})\n",
    "    \n",
    "\n",
    "\n",
    "    return z\n",
    "\n",
    "def tabulate_wk(G,z,c):\n",
    "    # This function tabulates the w_rs and kappa_r auxiliary data structures for the DC-SBM\n",
    "    #\n",
    "    # input  : G is simple graph with n nodes\n",
    "    #        : z is a dictionary of group labels for G's nodes, into c groups\n",
    "    #        : c is scalar, number of possible groups\n",
    "    # output : wrs, kpr\n",
    "    # \n",
    "    # WARNING: function is optimistic: assumes inputs are properly formatted\n",
    "    \n",
    "    wrs = np.zeros([c,c]) # count of stubs from group r to group s\n",
    "    kpr = np.zeros([c,1]) # total degree of group r\n",
    "\n",
    "    \n",
    "    \n",
    "#     calculated wrs by looping through edges to see if dictionary between the two is same or differnt. \n",
    "#     If same it add 2 to that index to account for stubs on both sides of edge. if different adds one to each of the indexs where the groups appear.\n",
    "    for e in list(G.edges()):\n",
    "        if z[e[0]] == z[e[1]]:\n",
    "            wrs[z[e[0]]][z[e[1]]]+= 2\n",
    "        else:\n",
    "            wrs[z[e[0]]][z[e[1]]]+= 1\n",
    "            wrs[z[e[1]]][z[e[0]]]+= 1\n",
    "#     Calculates kpr by summing each column in the wrs matrix and updates the index at that column index\n",
    "    for i in range(c):\n",
    "        summ = 0\n",
    "        for j in range(c):\n",
    "            summ = summ + wrs[i][j]\n",
    "        kpr[i]=summ\n",
    "\n",
    "\n",
    "\n",
    "    return wrs,kpr\n",
    "\n",
    "def dcsbm_LogL(wrs,kpr):\n",
    "    # This function calculates the log-likelihood of the degree-corrected stochastic block model (DC-SBM)\n",
    "\n",
    "    #\n",
    "    # input  : wrs is a c x c np.array of stub counts\n",
    "    #        : kpr is a c x 1 np.array of stub counts \n",
    "    # output : the dcsbm log-likelihood\n",
    "    # \n",
    "    # WARNING: function is optimistic: assumes inputs are properly formatted\n",
    "\n",
    "    c = wrs.shape[1]  # number of groups\n",
    "    \n",
    "    logL = 0\n",
    "    for r in range(c):\n",
    "        for s in range(c):\n",
    "            if wrs[r,s] < 1 or kpr[r] < 1 or kpr[s] < 1:\n",
    "                temp = 0 # define 0^0 = 1\n",
    "            else:\n",
    "                temp = wrs[r,s]*np.log( wrs[r,s] / (kpr[r]*kpr[s]) )\n",
    "            logL = logL + temp\n",
    "    \n",
    "    return logL\n",
    "\n",
    "def makeAMove(G,z,c,f):\n",
    "    # For each non 'frozen' node in the current partition, this function tries all (c-1) possible group moves for it\n",
    "    # It returns the combination of [node i and new group r] that produces the best log-likelihood over the non-frozen set\n",
    "    # input  : G, a graph\n",
    "    #        : z, a partition of G's nodes\n",
    "    #        : c, the number of groups\n",
    "    #        : f, a binary labeling of frozen nodes\n",
    "    # output : bestL, the best log-likelihood found\n",
    "    #        : bestMove, [i,r] the node i and new group r to achieve bestL\n",
    "    \n",
    "    bestL = -np.inf            # the best log-likelihood over all considered moves\n",
    "    for i in G.nodes():        # loop over all nodes i\n",
    "        if f[i] == 0:          # if i is not frozen\n",
    "            s = int(z[i])      #  get current label of i\n",
    "            for r in range(c): #  then loop over all groups r\n",
    "                # print(f'v[{i}] s = {s}, r={r}, {r!=s}') # for debugging\n",
    "                \n",
    "                 \n",
    "#               Checks to make sure old group is not same as new group then copies z and updates z so node is in new group.\n",
    "                if r!=s:\n",
    "                    zp = z.copy()\n",
    "                    zp.update({i: r})\n",
    "#               Runs Tabulate function and calculates Log likelyhood value and if its better than the previous best it saves the new one as the new best.\n",
    "                    wrs,kpr = tabulate_wk(G,zp,c)\n",
    "                    ll = dcsbm_LogL(wrs,kpr)\n",
    "                    if ll > bestL:\n",
    "                        bestL = ll\n",
    "                        bests = s\n",
    "                        bestMove = [i,r]\n",
    "    # print(f'v[{bestMove[0]}] g[{int(bests)}] --> g[{bestMove[1]}] : {bestL}')\n",
    "                \n",
    "                \n",
    "                \n",
    "    return bestL,bestMove\n",
    "\n",
    "def run_OnePhase(G,z0,c):\n",
    "    # Runs one phase, initialized from partition z0\n",
    "    # Returns the best partition found in the phase and the list of LogL values for all the phase's partitions\n",
    "    # input  : G, a graph\n",
    "    #        : z0, initial partition of G's nodes\n",
    "    #        : c, the number of groups\n",
    "    # output : zstar, the best partition of the phase\n",
    "    #        : Lstar, the LogL of zstar\n",
    "    #        : LL, the inorder list of LogL values for the n+1 partitions of this phase\n",
    "    #        : halt, 1 if zstar=z0 (no better partition found)\n",
    "\n",
    "    import copy      # for copy.deepcopy() function\n",
    "    n    = G.order() # n, number of nodes\n",
    "    LL   = []        # stores log-likelihoods over the entire algorithm (via .append)\n",
    "    halt = 0         # flag: =0 if Lstar > L0 at the end of the phase; =1 if Lstar <= L0\n",
    "\n",
    "    # initialize the phase\n",
    "    wrs,kpr = tabulate_wk(G,z0,c)      # wrs, kpr, initial DC-SBM parameters\n",
    "    L0      = dcsbm_LogL(wrs,kpr)      # store initial DC-SBM log-likelihood\n",
    "    LL.append(L0)                      # track log-likelihood\n",
    "\n",
    "    f     = dict.fromkeys(range(n), 0) # initially, all nodes unfrozen (tricky python)\n",
    "    t     = 0                          # number of frozen nodes in this phase\n",
    "    Lstar = L0                         # initially, z0 has the best LogL\n",
    "    zstar = copy.deepcopy(z0)          # and z0 is the best partition\n",
    "    tstar = t                          # tstar = 0\n",
    "\n",
    "    # loop over all the nodes in G, making greedy move for each\n",
    "    zt = copy.deepcopy(z0)             # start the loop at z0\n",
    "    for j in range(n):\n",
    "        # print(f'step {j}') # for debugging\n",
    "        # print(f)\n",
    "\n",
    "#       Gets best move and updates f to freeze the node moved and then copies zt and updates the copy of zt with the best move\n",
    "        choiceL,choiceMove = makeAMove(G,zt,c,f)\n",
    "        f.update({choiceMove[0]: 1})\n",
    "        zt = copy.deepcopy(zt)\n",
    "        zt.update({choiceMove[0]: choiceMove[1]})\n",
    "#       Adds log likelyhood value to LL list and then if the new LL is better than previous best LL then update the and save these best values as Lstar and Zstar\n",
    "        LL.append(choiceL)\n",
    "        if choiceL > Lstar:\n",
    "            Lstar = choiceL\n",
    "            zstar = copy.deepcopy(zt)\n",
    "        if Lstar == L0:\n",
    "            halt = 1\n",
    "        ##### do not modify below here #####\n",
    "\n",
    "    return zstar,Lstar,LL,halt\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def fit_DCSBM(G,c,T):\n",
    "    # Runs the full locally greedy heuristic, with c groups\n",
    "    # Returns the best partition found, its LogL, and the list of LogL values for all partitions considered\n",
    "    # input  : G, a graph\n",
    "    #        : c, the number of groups\n",
    "    #        : T, the number maximum number of phases allowed\n",
    "    # output : zstar, the best partition of the phase\n",
    "    #        : Lstar, the LogL of zstar\n",
    "    #        : LL, the inorder list of all LogL values considered\n",
    "    #        : pc, the number of phases in LL\n",
    "\n",
    "    import copy # for copy.deepcopy()\n",
    "    \n",
    "    # 1.0 locally greedy heuristic setup\n",
    "    n  = G.order() # n, number of nodes\n",
    "    LL   = []      # log-likelihoods over the entire algorithm (concat via .extend)\n",
    "    halt = 0       # convergence flag\n",
    "\n",
    "    # 2.0 generate initial partition, calculate wrs,kpr, and store the loglikelihood in Lt\n",
    "    zt      = random_z(n,c)       # z0, initial partition\n",
    "    wrs,kpr = tabulate_wk(G,zt,c) # wrs, kpr, initial DC-SBM parameters\n",
    "    Lt      = dcsbm_LogL(wrs,kpr) # store initial DC-SBM log-likelihood\n",
    "\n",
    "    # 3.0 the main loop\n",
    "    pc = 0  # counter for number of phases completed\n",
    "    while not halt:\n",
    "        # 3.1 visualization of this phase's initial partition\n",
    "        # print(f'phase[{pc}] z[0], logL = {Lt}')\n",
    "        # drawGz(G,zt)\n",
    "\n",
    "\n",
    "\n",
    "        zstar,Lstar,PLL,halt = run_OnePhase(G,zt,c)\n",
    "        zt = copy.deepcopy(zstar)\n",
    "        Lt = Lstar\n",
    "        LL.extend(PLL)\n",
    "        pc = pc + 1\n",
    "        if pc == T:\n",
    "            return\n",
    "       \n",
    "    \n",
    "    print(f' --> WE HAVE CONVERGENCE <-- ') # a friendly alert\n",
    "    return zstar,Lstar,LL,pc\n",
    "\n",
    "def drawGz(G,z):\n",
    "    \n",
    "    # This function draws G with node labels from partition z\n",
    "    #\n",
    "    # input  : G is a networkx graph\n",
    "    #        : z is a dictionary of group labels for G's nodes\n",
    "    # output : none\n",
    "    # \n",
    "    # WARNING: function is optimistic: assumes inputs are properly formatted\n",
    "\n",
    "    colors = ['#d61111','#11c6d6','#d67711','#11d646','#1b11d6','#d611cc'] # map node labels to colors (for the visualization)\n",
    "\n",
    "    node_colors = []\n",
    "    for i in G.nodes():\n",
    "        node_colors.append(colors[int(z[i])])\n",
    "    nsize  = 600\n",
    "    flabel = True\n",
    "\n",
    "    if G.order() > 50:\n",
    "        nsize  = 100\n",
    "        flabel = False\n",
    "        \n",
    "    nx.draw_networkx(G,with_labels=flabel,node_size=nsize,width=2,node_color=node_colors) # draw it pretty\n",
    "    limits=plt.axis('off')                                      # turn off axes\n",
    "    plt.show() \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Go = nx.convert_node_labels_to_integers(G) # map node names to integers (0:n-1) [because indexing]\n",
    "\n",
    "c    = 2       # c, number of groups\n",
    "T    = 30      # maximum number of phases; HALT if pc >= T\n",
    "reps = 20     # number of repetitions of fit_DCSBM()\n",
    "\n",
    "Lbest = -np.inf\n",
    "Zbest = ([])\n",
    "\n",
    "\n",
    "# Loops over number of reps and finds best LogLikelyhood value returned by fit_DCSBM and draws the resulting network.\n",
    "for i in range(reps):\n",
    "    zstar,Lstar,LL,pc = fit_DCSBM(Go,c,T)\n",
    "    if Lstar > Lbest:\n",
    "        Lbest = Lstar\n",
    "        Zbest = zstar\n",
    "drawGz(Go,Zbest)\n",
    "print(\"Best LL:\", Lbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b0d6d",
   "metadata": {},
   "source": [
    "Here we have the Local Smoothing algorithm detailed by this paper: https://proceedings.neurips.cc/paper/2021/file/a9eb812238f753132652ae09963a05e9-Paper.pdf\n",
    "\n",
    "This algorithm provides node attribute prediction based on the nodes it is in connection with. This code is from some of my coursework so this should be used as a tool and less to be built into a the product itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLabel_baseline(x):\n",
    "    # input:  x, dict of observed labels\n",
    "    # output: baseline predictor, Uniform(\\vec{x}-\\emptyset)\n",
    "    non_missing = []\n",
    "    for i in x:\n",
    "        if x[i]!= -1:\n",
    "            non_missing.append(x[i])\n",
    "    return np.random.choice(non_missing)\n",
    "\n",
    "def predictLabel_local(G,i,x,flag):\n",
    "    \n",
    "    # input:  G, simple networkx graph\n",
    "    #         i, a node in G whose label we will predict\n",
    "    #         x, dict of observed labels for G\n",
    "    #         flag, binary value\n",
    "    # output: local smoothing predictor output for i\n",
    "    #         a print statement (see instructions) if flag=1\n",
    "    if x[i] == -1:\n",
    "        counts = {}\n",
    "        neighbors = [n for n in G.neighbors(i)]\n",
    "        neighbor_attributes = []\n",
    "        for j in neighbors:\n",
    "            if x[j] != -1:\n",
    "                if not x[j] in neighbor_attributes:\n",
    "                    neighbor_attributes.append(x[j])\n",
    "                    counts[x[j]] = 1\n",
    "                else: \n",
    "                    counts[x[j]]+=1\n",
    "        if neighbor_attributes:\n",
    "            r = [g for g,l in counts.items() if l==max(counts.values())]\n",
    "            if len(r) == 1:\n",
    "                if flag == 1:\n",
    "                    print(\"node\",i, \": -1 -> \", r[0], \"(Smoothing)\")\n",
    "                return r[0]\n",
    "            else:\n",
    "                r = np.random.choice(r)\n",
    "                if flag == 1:\n",
    "                    print(\"node\",i, \": -1 -> \", r[0], \"(Smoothing)\")\n",
    "                return r\n",
    "        else:\n",
    "            base = predictLabel_baseline(x)\n",
    "            if flag == 1:\n",
    "                print(\"node\",i, \": -1 -> \", base, \"(Baseline)\")\n",
    "            return(base)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6732faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = x.copy()\n",
    "for i in x:\n",
    "    new_Att = predictLabel_local(Go,i,x,1)\n",
    "    if new_Att != None:\n",
    "        xp[i] = new_Att"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
